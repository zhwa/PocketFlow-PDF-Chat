"""
PocketFlow-based Local PDF Chat RAG System
Converted from Local_Pdf_Chat.txt to use PocketFlow components
"""
import sys
import os
import gradio as gr
from flow import create_offline_flow, create_online_flow
from utils.api_config import check_environment, check_serpapi_key

def create_shared_store():
    """Create the shared data store for the entire RAG system"""
    return {
        # Document processing
        "files": [],
        "extracted_texts": [],
        "chunks": [],
        "chunk_metadatas": [],
        "chunk_ids": [],
        
        # Embedding and indexing
        "embeddings": None,
        "faiss_index": None,
        "faiss_contents_map": {},
        "faiss_metadatas_map": {},
        "faiss_id_order": [],
        
        # BM25 indexing  
        "bm25_manager": None,
        
        # Query processing
        "query": "",
        "query_embedding": None,
        "enable_web_search": False,
        "model_choice": "ollama",
        
        # Retrieval results
        "semantic_results": [],
        "bm25_results": [],
        "hybrid_results": [],
        "reranked_results": [],
        "web_results": [],
        
        # Final output
        "context": "",
        "answer": "",
        "sources": [],
        
        # Configuration
        "config": {
            "chunk_size": 400,
            "chunk_overlap": 40,
            "embedding_model": "all-MiniLM-L6-v2",
            "rerank_method": "cross_encoder",
            "hybrid_alpha": 0.7,
            "max_iterations": 3,
            "top_k": 10,
            "final_k": 5
        }
    }

def process_pdfs_and_answer(files, question, enable_web_search=False, model_choice="ollama"):
    """Main function to process PDFs and answer questions"""
    if not files:
        return "è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶", None, "âŒ æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"
    
    if not question.strip():
        return "è¯·è¾“å…¥é—®é¢˜", None, "âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º"
    
    # Create shared store
    shared = create_shared_store()
    shared["files"] = files
    shared["query"] = question
    shared["enable_web_search"] = enable_web_search
    shared["model_choice"] = model_choice
    
    try:
        # Create flows
        offline_flow = create_offline_flow()
        online_flow = create_online_flow()
        
        # Run offline flow (document processing and indexing)
        print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£...")
        offline_flow.run(shared)
        
        # Run online flow (query processing and answer generation)
        print("ğŸ” å¼€å§‹å›ç­”é—®é¢˜...")
        online_flow.run(shared)
        
        # Format results
        answer = shared.get("answer", "æœªèƒ½ç”Ÿæˆå›ç­”")
        sources = shared.get("sources", [])
        
        # Create chat history format for Gradio (messages format)
        chat_history = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer}
        ]
        
        # Create processing status
        total_chunks = len(shared.get("chunks", []))
        processed_files = len([f for f in files if f is not None])
        status = f"âœ… æˆåŠŸå¤„ç† {processed_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        
        return chat_history, "", status
        
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        error_chat = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": f"âŒ {error_msg}"}
        ]
        return error_chat, "", f"âŒ {error_msg}"

def create_gradio_interface():
    """Create the Gradio web interface"""
    
    with gr.Blocks(title="PocketFlow PDF Chat", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ“š PocketFlow PDF Chat RAG System")
        gr.Markdown("åŸºäºPocketFlowæ¡†æ¶çš„æœ¬åœ°PDFé—®ç­”ç³»ç»Ÿ")
        
        with gr.Row():
            # Left panel - File upload and settings
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
                file_input = gr.File(
                    label="ä¸Šä¼ PDFæ–‡æ¡£",
                    file_types=[".pdf"],
                    file_count="multiple"
                )
                
                gr.Markdown("## âš™ï¸ è®¾ç½®")
                web_search_checkbox = gr.Checkbox(
                    label="å¯ç”¨è”ç½‘æœç´¢", 
                    value=False,
                    info="éœ€è¦é…ç½®SERPAPI_KEY"
                )
                
                model_choice = gr.Dropdown(
                    choices=["ollama", "siliconflow"],
                    value="ollama",
                    label="æ¨¡å‹é€‰æ‹©",
                    info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                )
                
                gr.Markdown("## ğŸ” æé—®")
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=3,
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."
                )
                
                ask_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                
            # Right panel - Chat and status
            with gr.Column(scale=2):
                gr.Markdown("## ğŸ’¬ å¯¹è¯")
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                    type="messages"
                )
                
                status_display = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    lines=2
                )
        
        # Environment status
        with gr.Row():
            gr.Markdown("### ğŸ”§ ç¯å¢ƒçŠ¶æ€")
            with gr.Column():
                env_status = gr.HTML()
                
                def check_env_status():
                    ollama_ok = check_environment()
                    serpapi_ok = check_serpapi_key()
                    
                    status_html = f"""
                    <div style="padding: 10px; border-radius: 5px; background: #f0f0f0;">
                        <p><strong>OllamaæœåŠ¡:</strong> {'âœ… æ­£å¸¸' if ollama_ok else 'âŒ å¼‚å¸¸'}</p>
                        <p><strong>SERPAPIé…ç½®:</strong> {'âœ… å·²é…ç½®' if serpapi_ok else 'âŒ æœªé…ç½®'}</p>
                    </div>
                    """
                    return status_html
                
                demo.load(check_env_status, outputs=env_status)
        
        # Bind events
        ask_btn.click(
            fn=process_pdfs_and_answer,
            inputs=[file_input, question_input, web_search_checkbox, model_choice],
            outputs=[chatbot, question_input, status_display]
        )
    
    return demo

if __name__ == "__main__":
    # Check environment before starting
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨")
        print("å¯åŠ¨å‘½ä»¤: ollama serve")
        print("æ¨¡å‹å®‰è£…: ollama pull deepseek-r1:1.5b")
        sys.exit(1)
    
    # Create and launch interface
    demo = create_gradio_interface()
    
    # Launch with auto port selection
    ports = [17995, 17996, 17997, 17998, 17999]
    for port in ports:
        try:
            demo.launch(
                server_port=port,
                server_name="0.0.0.0",
                show_error=True,
                share=False
            )
            break
        except Exception as e:
            if port == ports[-1]:  # Last port
                print(f"âŒ æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨: {e}")
                sys.exit(1)
            continue
